{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c60b66-7b38-4e30-a7f0-dc37873d8f13",
   "metadata": {},
   "source": [
    "# Working with the GPT-3.5-Turbo and GPT-4 models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429d764-4d64-46f7-acd0-8d627d461db4",
   "metadata": {},
   "source": [
    "## Getting an API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bc63a-e1d2-47ff-9e1b-4eddbdcf1976",
   "metadata": {},
   "source": [
    "The API empowers you with greater control and versatility to work with ChatGPT's responses. It also allows seamless integration with other applications.\n",
    "\n",
    "To access the model through the API, you will need an API key. For this demo, you'll be using your LT's API KEY.\n",
    "\n",
    "*To obtain your own API key, you'll need to create an account and set up billing. You can create your account at https://platform.openai.com/.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "040e7cbb-d253-46c0-a9da-c59fd9209fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "220729e3-1a25-4f08-943f-afaebac74fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative\n",
    "# api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f010fc3d-46c4-48d6-b420-6de136439e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative\n",
    "import os\n",
    "\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705056f-6395-4983-85fc-a56e06a1caff",
   "metadata": {},
   "source": [
    "## Getting openai Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac44c1-7500-4f77-b5b4-0a8f467c669d",
   "metadata": {},
   "source": [
    "To utilize the ChatGPT API, you'll need to have the openai Python package installed. \n",
    "\n",
    "You can easily install it by running the command pip install --upgrade openai. *Adding the --upgrade flag ensures that you have the most up-to-date version, as the ChatGPT API is a recently introduced feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae581f1d-c234-4afb-9040-5545661cae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "73da737a-e8a0-4784-8493-c5a10c1ba7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# load and set our key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af63c0-b790-4170-99b4-d99f29721a7c",
   "metadata": {},
   "source": [
    "## Chat Completions API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c3462-715c-47b0-9ff4-9443958f16ac",
   "metadata": {},
   "source": [
    "Chat models take as mandatory parameters:\n",
    "- **List of messages as input**\n",
    "- **Model**: we will use gpt-3.5-turbo\n",
    "\n",
    "They return a **model-generated message as output**. \n",
    "\n",
    "An example API call looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "42ce9a9c-78cf-44d4-9fe2-dc1f22e6ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[ # messages parameter must be a list of dictionaries\n",
    "    # can be as short as one message or many back and forth turns.  \n",
    "    {\"role\": \"system\", \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"}, # each dictionary has a role and content\n",
    "    {\"role\": \"user\", \"content\": \"What are the ingredients for making pancakes?\"},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacaa46-141f-488b-a062-cdc68e853fb4",
   "metadata": {},
   "source": [
    "In conversations using the Chat Completion API, each message has a **role (\"system,\" \"user,\" or \"assistant\").** Typically, a conversation starts with a system message to set the assistant's behavior, followed by alternating user and assistant messages. \n",
    "- The system message is optional and can be used to customize the assistant's personality or provide specific instructions\n",
    "- User messages contain requests or comments\n",
    "- Assistant messages store previous assistant responses or serve as examples of desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9de3c0-f9cf-488f-83e2-2dfa96a92b10",
   "metadata": {},
   "source": [
    "### Chat completions response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3c1d1665-3e06-45a7-9035-9929b93d3d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make delicious pancakes, you'll need the following ingredients:\n",
      "\n",
      "1. All-purpose flour: Use around 1 ½ cups of flour for a basic pancake recipe.\n",
      "2. Baking powder: About 1 ½ teaspoons will help the pancakes rise.\n",
      "3. Salt: Add a pinch of salt to enhance the flavor of the pancakes.\n",
      "4. Sugar: If you prefer a slightly sweet pancake, add 1-2 tablespoons of sugar.\n",
      "5. Milk: Use around 1 ¼ cups of milk to create a batter consistency.\n",
      "6. Eggs: Include 1 or 2 eggs to bind the ingredients together.\n",
      "7. Butter: Melt around 2 tablespoons of butter to add richness to the pancakes.\n",
      "8. Cooking oil: A small amount of oil for greasing the pan.\n",
      "\n",
      "These ingredients will make a classic pancake batter. Feel free to experiment and add flavors like vanilla extract, cinnamon, or even chocolate chips for variations.\n"
     ]
    }
   ],
   "source": [
    "# In Python, the assistant’s reply can be extracted with response['choices'][0]['message']['content'].\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefb398-23c9-42e7-aed3-e23b96df0a7c",
   "metadata": {},
   "source": [
    "Lets look at the whole response as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7477c92c-8761-4aac-8a5a-c44db8515079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7b9l1czjTkgWYadvkGsemR1lrUcGG at 0x7ff5001bb090> JSON: {\n",
       "  \"id\": \"chatcmpl-7b9l1czjTkgWYadvkGsemR1lrUcGG\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1689089999,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"To make delicious pancakes, you'll need the following ingredients:\\n\\n1. All-purpose flour: Use around 1 \\u00bd cups of flour for a basic pancake recipe.\\n2. Baking powder: About 1 \\u00bd teaspoons will help the pancakes rise.\\n3. Salt: Add a pinch of salt to enhance the flavor of the pancakes.\\n4. Sugar: If you prefer a slightly sweet pancake, add 1-2 tablespoons of sugar.\\n5. Milk: Use around 1 \\u00bc cups of milk to create a batter consistency.\\n6. Eggs: Include 1 or 2 eggs to bind the ingredients together.\\n7. Butter: Melt around 2 tablespoons of butter to add richness to the pancakes.\\n8. Cooking oil: A small amount of oil for greasing the pan.\\n\\nThese ingredients will make a classic pancake batter. Feel free to experiment and add flavors like vanilla extract, cinnamon, or even chocolate chips for variations.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 33,\n",
       "    \"completion_tokens\": 192,\n",
       "    \"total_tokens\": 225\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19981b-1908-4184-aab9-fc77a369e143",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Every response includes a finish_reason. The possible values for finish_reason are:\n",
    "\n",
    "- stop: API returned complete model output.\n",
    "- length: Incomplete model output due to max_tokens parameter or token limit.\n",
    "- content_filter: Omitted content due to a flag from our content filters.\n",
    "- null:API response still in progress or incomplete.\n",
    "\n",
    "Consider setting max_tokens to a slightly higher value than normal such as 300 or 500. This ensures that the model doesn't stop generating text before it reaches the end of the message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d21189-e888-4f55-b42d-cf7e7b6a2547",
   "metadata": {},
   "source": [
    "### Conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e62b68-2c6d-4825-92dc-4cf79fdcbb26",
   "metadata": {},
   "source": [
    "Since it recommended Baking Powder, let's ask how much in another prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3fa6ffe1-be5d-4217-b1b5-a5c25029db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[ \n",
    "    {\"role\": \"user\", \"content\": \"How much Baking Powder do you recommend for this recipe?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dd4bf4ca-6025-45a6-96d4-ec14b59dfb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide the correct amount of baking powder for your recipe, I would need specific details about the ingredients and quantities mentioned in the recipe. Could you please provide more information or share the recipe?\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5f3d8-11d0-49d8-88fd-5b47b2789f9d",
   "metadata": {},
   "source": [
    "We can see that the history was not saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85b63e-475b-4330-8c8e-7791074e7dbb",
   "metadata": {},
   "source": [
    "**Including conversation history is crucial** when user instructions refer to prior messages. We can do that by sending all the prompts, with its role, in the *messages* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d95a8d4d-921e-4331-89a8-3bda3576af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the ingredients for making pancakes?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make pancakes, you'll need flour, eggs, milk, baking powder, and a pinch of salt.\"}, # we shortened it for our demo purpose\n",
    "    {\"role\": \"user\", \"content\": \"Can I use almond milk instead of regular milk?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "108988dd-38a2-428b-a0e3-9d18613da8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! You can use almond milk as a substitute for regular milk in pancake recipes. Just keep in mind that almond milk may slightly alter the flavor and texture of the pancakes compared to using regular milk. Enjoy experimenting!\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3508d-c5e3-4475-b32f-7886984beb60",
   "metadata": {},
   "source": [
    "**Each user instruction relies on the prior messages in the conversation history to make sense.**\n",
    "\n",
    "Since the language models don't have inherent memory of past requests, it's important to include the relevant conversation history in each API request. If the conversation exceeds the model's token limit, it may need to be truncated or shortened while ensuring the essential context and instructions are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a446a56-43b9-47de-9e1d-d69ea9051219",
   "metadata": {},
   "source": [
    "### Creating a basic conversation loop\n",
    "\n",
    "This example demonstrates a conversation loop that performs the following tasks:\n",
    "\n",
    "1. Takes console input continuously and formats it as the user's role content within the messages array.\n",
    "2. Prints the model's response to the console and formats it as the assistant's role content within the messages array.\n",
    "\n",
    "This approach ensures that each time a new question is asked, the ongoing conversation transcript is sent along with the latest question. Since the model lacks memory, it's crucial to include an updated transcript with each new question. Otherwise, the model may lose context from previous questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d90675cd-823b-4c68-8bac-6d0149a72297",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def gpt_response(inp, message_history):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "    \n",
    "    # Generate a response from the chatbot model\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "      messages=message_history\n",
    "    )\n",
    "    \n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"}) \n",
    "    \n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "67bfa9fb-ce9a-4545-bea6-9a479a8edf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  i want to cook milanesas argentinas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Milanesas argentinas are delicious and quite simple to make. Here's a basic recipe to get you started:\n",
      "\n",
      "Ingredients:\n",
      "- 4 thinly sliced beef steaks (you can use veal, chicken, or pork as alternatives)\n",
      "- 2 eggs\n",
      "- 1 cup bread crumbs\n",
      "- Salt and pepper to taste\n",
      "- Vegetable oil for frying\n",
      "- Lemon wedges for serving\n",
      "\n",
      "Instructions:\n",
      "1. Place the beef steaks between two pieces of plastic wrap and pound them until they are about 1/4-inch thick.\n",
      "2. In a shallow dish, beat the eggs with salt and pepper.\n",
      "3. Place the bread crumbs on a separate plate.\n",
      "4. Dip each steak into the beaten eggs, coating it well on both sides.\n",
      "5. Then dredge the steak in the bread crumbs, pressing gently to make sure the crumbs adhere.\n",
      "6. Repeat this process with each steak.\n",
      "7. In a large frying pan, heat enough vegetable oil to cover the bottom of the pan.\n",
      "8. Fry the milanesas over medium heat for about 4-5 minutes on each side until golden brown.\n",
      "9. Once cooked, transfer them to a plate lined with paper towels to drain excess oil.\n",
      "10. Serve the milanesas hot with lemon wedges on the side.\n",
      "\n",
      "You can enjoy the milanesas alone or serve them with a side salad, fries, or mashed potatoes. Bon appétit!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  how big should the eggs be? S, M or L?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this recipe, you can use any size of eggs (S, M, or L). The size of the eggs won't affect the outcome of the dish. Use whichever size you have available.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/905hq7p50rz55pw6qwgr82_h0000gn/T/ipykernel_3028/3768474498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmessage_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Lets ask by input different prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Print the last response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"), message_history) # Lets ask by input different prompts\n",
    "    print(message_history[-1][\"content\"]) # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e41f0-c2c2-4ac7-ab3b-7adccb7b7a50",
   "metadata": {},
   "source": [
    "Let's check the message history to see if it has all the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3ee52100-283d-4745-b618-c24ac36a658d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'hi'},\n",
       " {'role': 'assistant', 'content': 'Hello! How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'i want to cook milanesas argentinas'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Great! Milanesas argentinas are delicious and quite simple to make. Here's a basic recipe to get you started:\\n\\nIngredients:\\n- 4 thinly sliced beef steaks (you can use veal, chicken, or pork as alternatives)\\n- 2 eggs\\n- 1 cup bread crumbs\\n- Salt and pepper to taste\\n- Vegetable oil for frying\\n- Lemon wedges for serving\\n\\nInstructions:\\n1. Place the beef steaks between two pieces of plastic wrap and pound them until they are about 1/4-inch thick.\\n2. In a shallow dish, beat the eggs with salt and pepper.\\n3. Place the bread crumbs on a separate plate.\\n4. Dip each steak into the beaten eggs, coating it well on both sides.\\n5. Then dredge the steak in the bread crumbs, pressing gently to make sure the crumbs adhere.\\n6. Repeat this process with each steak.\\n7. In a large frying pan, heat enough vegetable oil to cover the bottom of the pan.\\n8. Fry the milanesas over medium heat for about 4-5 minutes on each side until golden brown.\\n9. Once cooked, transfer them to a plate lined with paper towels to drain excess oil.\\n10. Serve the milanesas hot with lemon wedges on the side.\\n\\nYou can enjoy the milanesas alone or serve them with a side salad, fries, or mashed potatoes. Bon appétit!\\n\"},\n",
       " {'role': 'user', 'content': 'how big should the eggs be? S, M or L?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"For this recipe, you can use any size of eggs (S, M, or L). The size of the eggs won't affect the outcome of the dish. Use whichever size you have available.\"}]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d87d2a-f898-4864-8f89-74463bbccf5b",
   "metadata": {},
   "source": [
    "### Request parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e409d-2c31-4fed-bded-05573da8343e",
   "metadata": {},
   "source": [
    "Let's take a look at some request parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9c50f-7852-454a-ad30-69a7c23a2a48",
   "metadata": {},
   "source": [
    "- **Model**: Model type (e.g., GPT-3.5 turbo., GPT 4)\n",
    "- **Prompt**: expects a list of messages in a chat-based format\n",
    "- **Temperature (default 1)**: sampling temperature. Between 0 and 2. Higher value means more diverse and random output, lower is more while a lower value makes it more focused and deterministic.\n",
    "- **Max tokens (default 16)**: limits the length of the generated response (max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7cc0e101-8dbc-4673-b3fd-b012ad16eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets rewrite the gpt_response function to include possible parameters\n",
    "\n",
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "    \n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\", \n",
    "        \"messages\": message_history,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
    "    \n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"}) \n",
    "    \n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f677e-d938-4a93-8395-5a0476c3f823",
   "metadata": {},
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing three temperature levels (0, 1, and 2) to generate responses from the model, completing the prompt \"My favourite animal is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "61e662eb-2ec3-4620-a8a7-3a10b1e2358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the majestic and intelligent dolphin.\n",
      "\n",
      "the playful and adorable otter.\n",
      "\n",
      "the sleek and glorious panther.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    message_history = gpt_response(\"My favourite animal is \", \n",
    "                                   message_history, \n",
    "                                   max_tokens=100, \n",
    "                                   temperature=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a199af8-3690-4dd2-b6d3-9eeb3e852608",
   "metadata": {},
   "source": [
    "**top_p** (Defaults to 1)\n",
    "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "Higher value gives access to more tokens (and more diversity) and a lower value is more deterministic. \n",
    "\n",
    "We generally recommend altering this or temperature but not both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c39d3-9d50-4cc0-b39d-26410cfa3a51",
   "metadata": {},
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing two top_p levels (0 and 1) to generate responses from the model, completing the prompt \"My favourite animal is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "66e3323f-f839-45f9-a653-66d7be6af63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the majestic and intelligent dolphin.\n",
      "\n",
      "the playful and curious kitten.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1]:\n",
    "    message_history = gpt_response(\"My favourite animal is \", \n",
    "                                   message_history, \n",
    "                                   max_tokens=100, \n",
    "                                   top_p=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb50d3-2cc5-42b6-9af9-8101e826e349",
   "metadata": {},
   "source": [
    "**presence_penalty** (Defaults to 0)\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. **Higher values promote creativity by penalising the model when it uses predefined tokens.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3416cd-17f9-4be7-8098-b1fdb8177ede",
   "metadata": {},
   "source": [
    "**frequency_penalty** (Defaults to 0)\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. **Higher values penalise the model for repetition and reward variety.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c356ccf-0eb6-4ff7-afd6-9b1d8f958b0c",
   "metadata": {},
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing two presence penalty and frequency penalty levels (-2 and 2) to generate responses from the model, using the prompt \"generate 20 ways to say you can't buy that because you're broke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5e17a621-13b1-4afa-a528-3e70fd5071e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I'm sorry, I'm financially unable to purchase that right now.\n",
      "2. I'm afraid I'm unable to afford that at the moment.\n",
      "3. I'm unable to buy that due to financial constraints.\n",
      "4. I'm sorry, I'm financially unable to make that purchase.\n",
      "5. I'm unable to purchase that I'm I'm I'm I I I I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "\n",
      "1. Unfortunately, I don't have the funds to buy that.\n",
      "2. Regrettably, my current financial situation doesn't allow me to make this purchase.\n",
      "3. Apologies, but my budget is too tight for buying that right now.\n",
      "4. I'm afraid my wallet won't stretch far enough to afford it.\n",
      "5. Due to a lack of money, purchasing that isn't an option for me at the moment.\n",
      "\n",
      "6. Sadly, I can’t splurge on that because money\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[]\n",
    "\n",
    "for i in [-2,2]:\n",
    "    message_history = gpt_response(\"generate 20 ways to say you can't buy that because you're broke\",\n",
    "                                   message_history, \n",
    "                                   max_tokens=100, \n",
    "                                   presence_penalty=i, \n",
    "                                   frequency_penalty=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\")# Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413709d5-199a-4afb-81e7-02486a7bf084",
   "metadata": {},
   "source": [
    "## Using Chat Completion for non-chat scenarios\n",
    "The Chat Completion API is designed to work with multi-turn conversations, but it also works well for non-chat scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6ecec-2568-4c64-ba66-31ade5729634",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f9fff-5608-400c-9222-d2f4e1ae8db1",
   "metadata": {},
   "source": [
    "Lets set up a sentiment analysis scenario where a user inputs a tweet, and the program generates responses using GPT, providing sentiment predictions until interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb85b49-1fa6-42cb-a770-1494dcfd0566",
   "metadata": {},
   "source": [
    "We will give it as *system* the task to decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "We include as user pre-promt the example \"I loved the new Batman movie! Sentiment:\", and an example assistant answer \"Positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d7ef458f-b55b-4d15-b5ce-7d26e40761af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a tweet: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  i woke up a bit stressed today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what a day\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  this is amazing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/905hq7p50rz55pw6qwgr82_h0000gn/T/ipykernel_3028/1914178110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     message_history = gpt_response(input(\"> \"), \n\u001b[0m\u001b[1;32m     10\u001b[0m                                    \u001b[0mmessage_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Lets give an example of sentiment analysis\n",
    "message_history=[{\"role\": \"system\", \"content\": \"Decide whether a Tweet's sentiment is positive, neutral, or negative.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Tweet: \\\"I loved the new Batman movie!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Positive\"}\n",
    "  ]\n",
    "print(\"Write a tweet: \")\n",
    "\n",
    "while(True): \n",
    "    message_history = gpt_response(input(\"> \"), \n",
    "                                   message_history, \n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5fdca-17c4-4910-9e68-54a5922c5fc1",
   "metadata": {},
   "source": [
    "### Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d297f56-e4ae-48c5-a5bc-7b9e3470881f",
   "metadata": {},
   "source": [
    "Lets set up a translation scenario where a user inputs a phrase to be translated into Spanish and Portuguese. The program generates responses using GPT, providing translations until interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1b764-5cf8-4e5b-8fe4-35a62c92313f",
   "metadata": {},
   "source": [
    "We will give as a system pre-prompt *Translate this into Spanish, Portuguese, Italian*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "26a0fa03-7b43-446d-8118-2f49facb2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a phrase to translate to Spanish, Portuguese and Italian: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  whats up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: ¿Qué tal?\n",
      "Portuguese: Oi, tudo bem?\n",
      "Italian: Ciao, come stai?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/905hq7p50rz55pw6qwgr82_h0000gn/T/ipykernel_3028/1539288215.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmessage_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Translate this into Spanish, Portuguese, Italian\"}\n",
    "  ]\n",
    "print(\"Write a phrase to translate to Spanish, Portuguese and Italian: \")\n",
    "\n",
    "while(True): \n",
    "    message_history = gpt_response(input(\"> \"), message_history,temperature=0.3,max_tokens=60)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b193c4-956a-42aa-b00b-23ddb07865bb",
   "metadata": {},
   "source": [
    "# Extra: creating a chatbot with gradio for front-end UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ae714-b13a-42a3-a13a-6213ed7ff1b4",
   "metadata": {},
   "source": [
    "Gradio is a Python library that allows you to quickly create customizable UIs for machine learning models, or for any kind of Python function or code snippet, with just a few lines of code. It simplifies the process of deploying and sharing models or code by generating interactive interfaces for input and output data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14a415-9fe0-49f6-8693-4576d2c74ac7",
   "metadata": {},
   "source": [
    "To create a chatbot with Gradio for the front-end user interface, follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f1e1a-26f4-4b96-8fef-51436880d4a4",
   "metadata": {},
   "source": [
    "1. Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de7b14-5c2f-4456-8ef8-6907d2d96e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93819469-c55a-488f-b4fa-6d5e21e597a8",
   "metadata": {},
   "source": [
    "2. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "49384515-5e93-4f89-acbb-884154a8768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d1c73-f986-494e-96b7-77aa4ba421bf",
   "metadata": {},
   "source": [
    "3. Set up your OpenAI API credentials by assigning your API key to openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7b881-19b9-478c-8c5b-5bcd85c52a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3520acd-ed0e-47ee-b213-9b45142774e6",
   "metadata": {},
   "source": [
    "4. Define a function that generates chatbot responses based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ffd0385d-822a-42ab-8f8d-067ae2cc09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the conversation history with user and assistant messages\n",
    "message_history = [{\"role\": \"system\", \"content\": f\"Respond my prompts in a Harry Potter style\"},\n",
    "                   {\"role\": \"assistant\", \"content\": f\"OK\"}]\n",
    "    \n",
    "def gpt_response_ui(inp):\n",
    "    global message_history\n",
    "\n",
    "    message_history = gpt_response(inp, message_history, max_tokens = 50)\n",
    "\n",
    "    # Get pairs (as tuples) of msg[\"content\"] from message history,representing one exchange between the user and the chatbot, skipping the pre-prompt\n",
    "    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]  # convert to tuples of list\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42101a7-55bc-4052-9a5a-368d2472579d",
   "metadata": {},
   "source": [
    "5. Define the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "060004ad-f547-48a4-8d0e-b52ca38d957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo: #creates a Gradio interface\n",
    "\n",
    "    chatbot = gr.Chatbot() #creates a chatbot instance\n",
    "\n",
    "    with gr.Row(): #creates a row within the Gradio interface to contain components\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\") #text input field\n",
    "        # `show_label=False` parameter hides the label associated with the textbox\n",
    "    \n",
    "    txt.submit(gpt_response_ui, txt, chatbot) #sets the submit action to the `gpt_response` function\n",
    "    txt.submit(lambda :\"\", None, txt) #this clears the textbox when the user submits their input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68df5c2-c3e2-41a8-9b7b-ca6fdf4d600b",
   "metadata": {},
   "source": [
    "6. Run the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3398916f-bb1e-4eda-b9f2-73ceba2695e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "Running on public URL: https://171bfbe57efd741bc3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://171bfbe57efd741bc3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True) #To create a public link, set `share=True` in `launch()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d0ae4-659d-4324-a253-b31c54fdc97f",
   "metadata": {},
   "source": [
    "Source Inspiration: MIT License - Copyright (c) 2023 Harrison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
